{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb35680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened time series with Crop_Name saved to: Karnataka_Datasets/Across/SAR/SAR_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_dir = \"Karnataka_Datasets/Across/SAR/\"\n",
    "output_file = os.path.join(input_dir, \"SAR_Data.csv\")\n",
    "\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "dfs = []\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combine all into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Parse dates using mixed format\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date'], format='mixed', dayfirst=True, errors='coerce')\n",
    "combined_df = combined_df.dropna(subset=['date'])\n",
    "\n",
    "# Sort by date to maintain time order\n",
    "combined_df = combined_df.sort_values(by=['Latitude', 'Longitude', 'date'])\n",
    "\n",
    "# Assign time index within each group\n",
    "combined_df['time_index'] = combined_df.groupby(['Latitude', 'Longitude']).cumcount() + 1\n",
    "\n",
    "# Get the most frequent Crop_Name for each (Latitude, Longitude)\n",
    "crop_name_df = combined_df.groupby(['Latitude', 'Longitude'])['Crop_Name'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
    "crop_name_df = crop_name_df.reset_index()\n",
    "\n",
    "# Pivot VH\n",
    "vh_pivot = combined_df.pivot(index=['Latitude', 'Longitude'], columns='time_index', values='VH')\n",
    "vh_pivot.columns = [f'VH_{i}' for i in vh_pivot.columns]\n",
    "\n",
    "# Pivot VV\n",
    "vv_pivot = combined_df.pivot(index=['Latitude', 'Longitude'], columns='time_index', values='VV')\n",
    "vv_pivot.columns = [f'VV_{i}' for i in vv_pivot.columns]\n",
    "\n",
    "# Merge all parts\n",
    "result = pd.concat([crop_name_df.set_index(['Latitude', 'Longitude']), vh_pivot, vv_pivot], axis=1).reset_index()\n",
    "\n",
    "# Sort VH and VV columns numerically\n",
    "vh_columns = sorted([col for col in result.columns if col.startswith('VH_')], key=lambda x: int(x.split('_')[1]))\n",
    "vv_columns = sorted([col for col in result.columns if col.startswith('VV_')], key=lambda x: int(x.split('_')[1]))\n",
    "\n",
    "# Final column order\n",
    "final_columns = ['Latitude', 'Longitude', 'Crop_Name'] + vh_columns + vv_columns\n",
    "result = result[final_columns]\n",
    "\n",
    "# Save the result\n",
    "result.to_csv(output_file, index=False)\n",
    "print(f\"Flattened time series with Crop_Name saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12decf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: Karnataka_Datasets/Across/SAR/SAR_Data_Clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "input_file = \"Karnataka_Datasets/Across/SAR/SAR_Data_Clean.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Identify all VV and VH columns (from VV_1 to VV_31 and VH_1 to VH_31)\n",
    "vv_columns = [f'VV_{i}' for i in range(1, 32)]\n",
    "vh_columns = [f'VH_{i}' for i in range(1, 32)]\n",
    "\n",
    "# Combine all VV and VH columns into a list\n",
    "all_columns = vv_columns + vh_columns\n",
    "\n",
    "# Get the unique count of each column, then filter out those with only one unique value\n",
    "columns_to_keep = [col for col in all_columns if df[col].nunique() > 1]\n",
    "\n",
    "# Keep only the original columns (including Crop_Name, Latitude, and Longitude)\n",
    "df_cleaned = df[['Latitude', 'Longitude', 'Crop_Name'] + columns_to_keep]\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "output_file = \"Karnataka_Datasets/Across/SAR/SAR_Data_Clean.csv\"\n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "print(f\"Cleaned data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1aaa995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to: Karnataka_Datasets/Across/SAR/SAR_Mapped.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read both CSV files into DataFrames\n",
    "file2 = \"Karnataka_Datasets/Across/Merged_Karnataka_S2_Kharif_Mapped_Clean.csv\"  # File with the VV, VH columns\n",
    "file1 = \"Karnataka_Datasets/Across/SAR/SAR_Data_Clean.csv\"  # File with Structure and Structure_Numeric columns\n",
    "\n",
    "df1 = pd.read_csv(file1)  # First file\n",
    "df2 = pd.read_csv(file2)  # Second file\n",
    "\n",
    "# Merge the DataFrames based on Latitude and Longitude\n",
    "merged_df = pd.merge(df1, df2[['Latitude', 'Longitude', 'Structure', 'Structure_Numeric']], \n",
    "                     on=['Latitude', 'Longitude'], how='inner')\n",
    "\n",
    "# Save the merged DataFrame back to a new CSV file\n",
    "output_file = \"Karnataka_Datasets/Across/SAR/SAR_Mapped.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Merged data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30d5c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure_Numeric\n",
      "3    46564\n",
      "1    17055\n",
      "2    10186\n",
      "4      120\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the count of each unique value in the 'Crop_Name' column\n",
    "print(merged_df['Structure_Numeric'].value_counts())\n",
    "\n",
    "# If you're referring to a different column, for example, 'Class':\n",
    "# print(merged_df['Class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4382b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def balance_on_structure_numeric(csv_path, output_path=None):\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Step 1: Remove rows where Structure_Numeric == 4\n",
    "    df = df[df['Structure_Numeric'] != 4]\n",
    "\n",
    "    # Prepare final balanced dataframe\n",
    "    balanced_df = pd.DataFrame()\n",
    "\n",
    "    # Step 2: Loop over each unique Crop_Name\n",
    "    for crop in df['Crop_Name'].unique():\n",
    "        crop_df = df[df['Crop_Name'] == crop]\n",
    "\n",
    "        # Get the class distribution for this crop\n",
    "        class_counts = crop_df['Structure_Numeric'].value_counts()\n",
    "\n",
    "        # If there's only one class left, skip (nothing to balance)\n",
    "        if len(class_counts) < 2:\n",
    "            continue\n",
    "\n",
    "        # Find the minimum class count (for undersampling)\n",
    "        min_count = class_counts.min()\n",
    "\n",
    "        # Undersample each class to the minimum count\n",
    "        sampled_list = []\n",
    "        for structure_value in class_counts.index:\n",
    "            class_subset = crop_df[crop_df['Structure_Numeric'] == structure_value]\n",
    "            sampled = resample(class_subset, \n",
    "                               replace=False, \n",
    "                               n_samples=min_count, \n",
    "                               random_state=42)\n",
    "            sampled_list.append(sampled)\n",
    "\n",
    "        # Combine and add to final DataFrame\n",
    "        balanced_crop_df = pd.concat(sampled_list)\n",
    "        balanced_df = pd.concat([balanced_df, balanced_crop_df])\n",
    "\n",
    "    # Shuffle the final dataset\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Save if output path is provided\n",
    "    if output_path:\n",
    "        balanced_df.to_csv(output_path, index=False)\n",
    "\n",
    "    return balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e97ed383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure_Numeric\n",
      "3    46535\n",
      "1    17079\n",
      "2    10190\n",
      "4      120\n",
      "5        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Karnataka_Datasets/Across/SAR/SAR_Mapped.csv\")\n",
    "\n",
    "# Print the count of each unique value in Structure_Numeric\n",
    "print(df['Structure_Numeric'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2075c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final counts:\n",
      " Structure_Numeric\n",
      "1    17079\n",
      "3    13000\n",
      "2    10190\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final total rows: 40269\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"Karnataka_Datasets/Across/SAR/SAR_Mapped.csv\")\n",
    "\n",
    "# Remove Structure_Numeric == 4 or 5\n",
    "df = df[~df['Structure_Numeric'].isin([4, 5])]\n",
    "\n",
    "# Separate classes\n",
    "df_3 = df[df['Structure_Numeric'] == 3]\n",
    "df_1 = df[df['Structure_Numeric'] == 1]\n",
    "df_2 = df[df['Structure_Numeric'] == 2]\n",
    "\n",
    "# Downsample class 3 to 13,000\n",
    "df_3_downsampled = resample(df_3, \n",
    "                            replace=False, \n",
    "                            n_samples=13000, \n",
    "                            random_state=42)\n",
    "\n",
    "# Combine all\n",
    "balanced_df = pd.concat([df_3_downsampled, df_1, df_2])\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Final check\n",
    "print(\"Final counts:\\n\", balanced_df['Structure_Numeric'].value_counts())\n",
    "print(\"\\nFinal total rows:\", len(balanced_df))\n",
    "\n",
    "# Optional: Save\n",
    "final_df.to_csv(\"Karnataka_Datasets/Across/SAR/Karnataka_SAR_Balanced.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0573f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Crop_Name distribution:\n",
      " Crop_Name\n",
      "Ragi          0.280304\n",
      "Coconut       0.157504\n",
      "Rose          0.135930\n",
      "Mangoes       0.090486\n",
      "Arecanut      0.079094\n",
      "Avare         0.073817\n",
      "Guava         0.046904\n",
      "Redgram       0.043303\n",
      "Sapota        0.024274\n",
      "Banana        0.023778\n",
      "Jowar         0.019867\n",
      "Maize         0.018966\n",
      "Paddy         0.003352\n",
      "Lemon         0.001707\n",
      "Eucalyptus    0.000341\n",
      "Sugarcane     0.000217\n",
      "Bajra         0.000155\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test Crop_Name distribution:\n",
      " Crop_Name\n",
      "Ragi          0.280358\n",
      "Coconut       0.157561\n",
      "Rose          0.135957\n",
      "Mangoes       0.090514\n",
      "Arecanut      0.079091\n",
      "Avare         0.073752\n",
      "Guava         0.046933\n",
      "Redgram       0.043333\n",
      "Sapota        0.024212\n",
      "Banana        0.023839\n",
      "Jowar         0.019866\n",
      "Maize         0.018873\n",
      "Paddy         0.003352\n",
      "Lemon         0.001738\n",
      "Eucalyptus    0.000372\n",
      "Sugarcane     0.000124\n",
      "Bajra         0.000124\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming `balanced_df` is your DataFrame from before\n",
    "\n",
    "# Stratified 80-20 split based on Crop_Name\n",
    "train_df, test_df = train_test_split(\n",
    "    balanced_df,\n",
    "    test_size=0.2,\n",
    "    stratify=balanced_df['Crop_Name'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Confirm split\n",
    "print(\"Train Crop_Name distribution:\\n\", train_df['Crop_Name'].value_counts(normalize=True))\n",
    "print(\"\\nTest Crop_Name distribution:\\n\", test_df['Crop_Name'].value_counts(normalize=True))\n",
    "\n",
    "# Optional: Save\n",
    "train_df.to_csv(\"Karnataka_Datasets/Across/SAR/Karnataka_SAR_train.csv\", index=False)\n",
    "test_df.to_csv(\"Karnataka_Datasets/Across/SAR/Karnataka_SAR_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd3281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
