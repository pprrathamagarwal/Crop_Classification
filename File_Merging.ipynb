{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffeebaa9",
   "metadata": {},
   "source": [
    "#  All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a01be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "import ast\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a51f437",
   "metadata": {},
   "source": [
    "### Break File In Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d04aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/Mandya_1.csv\n",
      "Saved Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/Mandya_2.csv\n",
      "Saved Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/Mandya_3.csv\n",
      "Saved Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/Mandya_4.csv\n",
      "Saved Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/Mandya_5.csv\n",
      "Saved Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/Mandya_6.csv\n",
      "Saved Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/Mandya_7.csv\n",
      "Saved Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/Mandya_8.csv\n",
      "Saved Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/Mandya_9.csv\n",
      "Saved Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/Mandya_10.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the original CSV file\n",
    "file_path = r\"Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/data_2021-2022.csv\"  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Number of rows per chunk\n",
    "rows_per_chunk = 5000\n",
    "\n",
    "# Starting chunk number\n",
    "start_chunk = 1\n",
    "\n",
    "# Define the directory where the files should be saved\n",
    "output_dir = 'Karnataka_Datasets/Across/Kharif/Mandya/CSVs/data/2021-22/'  # Replace with your directory path\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the column headers (attributes)\n",
    "header = df.columns\n",
    "\n",
    "# Split the dataframe into chunks, and include the header in each chunk\n",
    "for i, chunk in enumerate(range(0, len(df), rows_per_chunk)):\n",
    "    # Extract the chunk of data\n",
    "    chunk_df = df.iloc[chunk:chunk + rows_per_chunk]\n",
    "    \n",
    "    # Save the chunk to a CSV file in the specified directory\n",
    "    chunk_file_name = os.path.join(output_dir, f\"Mandya_{start_chunk + i}.csv\")\n",
    "    chunk_df.to_csv(chunk_file_name, index=False, header=header)\n",
    "    print(f\"Saved {chunk_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd1090",
   "metadata": {},
   "source": [
    "### Remove Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6732fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows have been removed and the new file is saved as 'your_file_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the CSV file\n",
    "file_path = r\"C:\\Courses\\Minor Project\\Data Sheet\\Kharif\\Kharif_Dataset.csv\"  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Save the modified CSV file\n",
    "df_cleaned.to_csv(r\"C:\\Courses\\Minor Project\\Data Sheet\\Kharif\\Kharif_Dataset.csv\", index=False)\n",
    "\n",
    "print(\"Duplicate rows have been removed and the new file is saved as 'your_file_cleaned.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4c7e8",
   "metadata": {},
   "source": [
    "### Merge CSVs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c745e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the directory containing the CSV files\n",
    "directory = 'Karnataka_Datasets/Model_Data/Test_Chunks_Results/Processed_Files/Processed_Files_Dropped/'\n",
    "\n",
    "# List to hold the dataframes\n",
    "df_list = []\n",
    "\n",
    "# Loop over the CSV files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):  # Process only CSV files\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a dataframe and append it to the list\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('Karnataka_Datasets/Model_Data/Test_Chunks_Results/Processed_Files/Processed_Merged.csv', index=False)\n",
    "\n",
    "print(\"Merged CSV created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4abcc82",
   "metadata": {},
   "source": [
    "### Drop Rows with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384a960d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with empty cells have been removed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('interpolated_file_self_learn.csv')\n",
    "\n",
    "# Drop rows that contain any empty cell (NaN) within the column header range\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame back to a new CSV file\n",
    "df_cleaned.to_csv('output.csv', index=False)\n",
    "\n",
    "print(\"Rows with empty cells have been removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae96276",
   "metadata": {},
   "source": [
    "# Removing Unnecessary Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e0a1f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['interval_end_date'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Drop the specified columns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m columns_to_drop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval_start_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem:index\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.geo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval_end_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mcolumns_to_drop, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Save the modified DataFrame back to the original file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5198\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5208\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5209\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5342\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5345\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5346\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5347\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5348\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5349\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5350\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5351\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5352\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4711\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4751\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4753\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4754\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7000\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7000\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7001\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['interval_end_date'] not found in axis\""
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "file_path = 'Karnataka_Datasets/Atibelle_Extracted/final_base_file.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['interval_start_date', 'system:index', '.geo', 'interval_end_date']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame back to the original file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Columns {columns_to_drop} dropped and file saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f6793",
   "metadata": {},
   "source": [
    "### Crop Count and Their Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edce85b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragi: 13974 (68.18%)\n",
      "Coconut: 2186 (10.67%)\n",
      "Arecanut: 1402 (6.84%)\n",
      "Jowar: 927 (4.52%)\n",
      "Maize: 887 (4.33%)\n",
      "Redgram: 581 (2.83%)\n",
      "Banana: 328 (1.60%)\n",
      "Paddy: 171 (0.83%)\n",
      "Potato: 25 (0.12%)\n",
      "Bajra: 10 (0.05%)\n",
      "Wheat: 5 (0.02%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Karnataka_Datasets/Model_Data/Karnataka_test.csv')\n",
    "\n",
    "# Get crop frequency counts\n",
    "crop_counts = df['Crop_Name'].value_counts()\n",
    "\n",
    "# Calculate the percentage for each crop\n",
    "crop_percentages = (crop_counts / crop_counts.sum()) * 100\n",
    "\n",
    "# Set display option to show more rows\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Sort crops by frequency\n",
    "sorted_crops = crop_counts.sort_values(ascending=False)\n",
    "\n",
    "# Print each crop with its frequency and percentage\n",
    "for crop, freq in sorted_crops.items():\n",
    "    percentage = crop_percentages[crop]\n",
    "    print(f\"{crop}: {freq} ({percentage:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
