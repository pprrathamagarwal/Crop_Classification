{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49dd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ee\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00af63",
   "metadata": {},
   "source": [
    "# BREAK CSV INTO CHUNKS (ONLY IF NECESSARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b519ca1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_1.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_2.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_3.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_4.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_5.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_6.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_7.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_8.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_9.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_10.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_11.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_12.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_13.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_14.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_15.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_16.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_17.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_18.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_19.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_20.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_21.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_22.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_23.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_24.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_25.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_26.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_27.csv\n",
      "Saved Karnataka_Datasets/Across/Chunks\\chunk_28.csv\n"
     ]
    }
   ],
   "source": [
    "# Input CSV file path\n",
    "input_file = \"Karnataka_Datasets/Karnataka_Dataset_Across_Regions.csv\"\n",
    "\n",
    "# Output directory for chunks\n",
    "output_dir = \"Karnataka_Datasets/Across/Chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set the chunk size\n",
    "chunk_size = 5000\n",
    "\n",
    "# Read the CSV in chunks\n",
    "for i, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size)):\n",
    "    chunk_filename = os.path.join(output_dir, f\"chunk_{i+1}.csv\")\n",
    "    chunk.to_csv(chunk_filename, index=False)\n",
    "    print(f\"Saved {chunk_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09425005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Export started. Monitor progress at: https://code.earthengine.google.com/tasks\n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "input_csv_path = 'Karnataka_Datasets/Across/Chunks/chunk_10.csv'\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Convert to ee.FeatureCollection\n",
    "def row_to_feature(row):\n",
    "    point = ee.Geometry.Point([row['Longitude'], row['Latitude']])\n",
    "    buffer = point.buffer(10)\n",
    "    return ee.Feature(buffer, {\n",
    "        'Latitude': row['Latitude'],\n",
    "        'Longitude': row['Longitude'],\n",
    "        'Crop_Name': row['Crop_Name']\n",
    "    })\n",
    "\n",
    "features = [row_to_feature(row) for _, row in df.iterrows()]\n",
    "feature_collection = ee.FeatureCollection(features)\n",
    "\n",
    "# Dates\n",
    "start_date = datetime.date(2021, 4, 1)\n",
    "end_date = datetime.date(2022, 3, 31)\n",
    "delta = datetime.timedelta(days=5)\n",
    "\n",
    "dates = []\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    dates.append(current)\n",
    "    current += delta\n",
    "\n",
    "# Add indices function\n",
    "def add_indices(image):\n",
    "    # Divide the bands by 10000 to scale the reflectance values correctly\n",
    "    image = image.multiply(0.0001)  # This is equivalent to dividing by 10000\n",
    "    \n",
    "    # Add NDVI and GCVI indices\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    gcvi = image.select('B8').divide(image.select('B3')).subtract(1).rename('GCVI')\n",
    "    \n",
    "    return image.addBands([ndvi, gcvi])\n",
    "\n",
    "\n",
    "\n",
    "# List to hold feature collections for each date\n",
    "all_fc = []\n",
    "\n",
    "for date in dates:\n",
    "    start = ee.Date(str(date))\n",
    "    end = start.advance(5, 'day')\n",
    "\n",
    "    s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "        .filterDate(start, end) \\\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 90)) \\\n",
    "        .select(['B3', 'B4', 'B8', 'B8A', 'B11', 'B12'])  # Added SCL band\n",
    "\n",
    "    # Apply SCL cloud mask\n",
    "\n",
    "    img = s2.median()\n",
    "    img = add_indices(img)\n",
    "\n",
    "    band_names = img.bandNames().getInfo()\n",
    "    if not band_names:\n",
    "        print(f\"⚠️ No image found for {date}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    reduced = img.reduceRegions(\n",
    "        collection=feature_collection,\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        scale=10\n",
    "    )\n",
    "\n",
    "    # Tag each feature with the date\n",
    "    reduced = reduced.map(lambda f: f.set('Date', str(date)))\n",
    "    all_fc.append(reduced)\n",
    "\n",
    "# Merge all results\n",
    "merged_fc = ee.FeatureCollection(all_fc).flatten()\n",
    "\n",
    "# Export to Google Drive\n",
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection=merged_fc,\n",
    "    description='Karnataka_Across_Data',\n",
    "    folder='EarthEngine_VI',\n",
    "    fileNamePrefix='Karnataka_Chunk_10_90',\n",
    "    fileFormat='CSV'\n",
    ")\n",
    "task.start()\n",
    "\n",
    "print(\"✅ Export started. Monitor progress at: https://code.earthengine.google.com/tasks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2952c",
   "metadata": {},
   "source": [
    "# EXTRACT S2 DATA FOR A SINGLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f423e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Export started. Monitor progress at: https://code.earthengine.google.com/tasks\n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "input_csv_path = 'Karnataka_Datasets/Across/Chunks/chunk_10.csv'\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Convert to ee.FeatureCollection\n",
    "def row_to_feature(row):\n",
    "    point = ee.Geometry.Point([row['Longitude'], row['Latitude']])\n",
    "    buffer = point.buffer(10)\n",
    "    return ee.Feature(buffer, {\n",
    "        'Latitude': row['Latitude'],\n",
    "        'Longitude': row['Longitude'],\n",
    "        'Crop_Name': row['Crop_Name']\n",
    "    })\n",
    "\n",
    "features = [row_to_feature(row) for _, row in df.iterrows()]\n",
    "feature_collection = ee.FeatureCollection(features)\n",
    "\n",
    "# Dates\n",
    "start_date = datetime.date(2021, 4, 1)\n",
    "end_date = datetime.date(2022, 3, 31)\n",
    "delta = datetime.timedelta(days=5)\n",
    "\n",
    "dates = []\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    dates.append(current)\n",
    "    current += delta\n",
    "\n",
    "# Add NDVI and GCVI indices\n",
    "def add_indices(image):\n",
    "    image = image.multiply(0.0001)  # scale reflectance\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    gcvi = image.select('B8').divide(image.select('B3')).subtract(1).rename('GCVI')\n",
    "    return image.addBands([ndvi, gcvi])\n",
    "\n",
    "# QA60 cloud masking function\n",
    "def maskS2cloudsQA60(image):\n",
    "    qa = image.select('QA60')\n",
    "    cloud_bit_mask = 1 << 10  # Cloud\n",
    "    cirrus_bit_mask = 1 << 11  # Cirrus\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "           qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    return image.updateMask(mask)\n",
    "\n",
    "# List to hold feature collections for each date\n",
    "all_fc = []\n",
    "\n",
    "# Loop over all dates\n",
    "for date in dates:\n",
    "    start = ee.Date(str(date))\n",
    "    end = start.advance(5, 'day')\n",
    "\n",
    "    s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "        .filterDate(start, end) \\\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 50)) \\\n",
    "        .map(maskS2cloudsQA60) \\\n",
    "        .select(['B3', 'B4', 'B8', 'B8A', 'B11', 'B12', 'QA60'])  # Include QA60 for masking\n",
    "\n",
    "    img = s2.median()\n",
    "    img = add_indices(img)\n",
    "\n",
    "    band_names = img.bandNames().getInfo()\n",
    "    if not band_names:\n",
    "        print(f\"⚠️ No image found for {date}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    reduced = img.reduceRegions(\n",
    "        collection=feature_collection,\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        scale=10\n",
    "    )\n",
    "\n",
    "    reduced = reduced.map(lambda f: f.set('Date', str(date)))\n",
    "    all_fc.append(reduced)\n",
    "\n",
    "# Merge all results\n",
    "merged_fc = ee.FeatureCollection(all_fc).flatten()\n",
    "\n",
    "# Export to Google Drive\n",
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection=merged_fc,\n",
    "    description='Karnataka_Across_Data',\n",
    "    folder='EarthEngine_VI_QA_90',\n",
    "    fileNamePrefix='Karnataka_Chunk_10_QA',\n",
    "    fileFormat='CSV'\n",
    ")\n",
    "task.start()\n",
    "\n",
    "print(\"✅ Export started. Monitor progress at: https://code.earthengine.google.com/tasks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef7e39",
   "metadata": {},
   "source": [
    "# EXTRACT S2 DATA FOR A DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d18659b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_23\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_24\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_25\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_26\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_27\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_28\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_3\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_4\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_5\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_6\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_7\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_8\n",
      "⚠️ No image found for 2021-11-07, skipping.\n",
      "⚠️ No image found for 2021-12-12, skipping.\n",
      "✅ Export started for chunk_9\n"
     ]
    }
   ],
   "source": [
    "# Input directory containing all chunks\n",
    "input_dir = 'Karnataka_Datasets/Across/Chunks_Rem/'\n",
    "\n",
    "# Output settings\n",
    "export_folder = 'EarthEngine_VI_QA_CloudProb'\n",
    "export_description_prefix = 'Karnataka_Chunk'\n",
    "\n",
    "# Define dates\n",
    "start_date = datetime.date(2021, 4, 1)\n",
    "end_date = datetime.date(2022, 3, 31)\n",
    "delta = datetime.timedelta(days=5)\n",
    "\n",
    "dates = []\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    dates.append(current)\n",
    "    current += delta\n",
    "\n",
    "# NDVI & GCVI function\n",
    "def add_indices(image):\n",
    "    image = ee.Image(image).multiply(0.0001)\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    gcvi = image.select('B8').divide(image.select('B3')).subtract(1).rename('GCVI')\n",
    "    return image.addBands([ndvi, gcvi])\n",
    "\n",
    "# Cloud masking\n",
    "def mask_clouds(image):\n",
    "    qa60 = image.select('QA60')\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    qa_mask = qa60.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "              qa60.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    \n",
    "    cloud_mask_img = ee.Image(image.get('cloud_mask'))\n",
    "    \n",
    "    return ee.Algorithms.If(\n",
    "        cloud_mask_img,\n",
    "        image.updateMask(qa_mask.And(cloud_mask_img.select('probability').lt(50))),\n",
    "        image.updateMask(qa_mask)\n",
    "    )\n",
    "\n",
    "# Process each file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Convert to FeatureCollection\n",
    "    def row_to_feature(row):\n",
    "        point = ee.Geometry.Point([row['Longitude'], row['Latitude']])\n",
    "        return ee.Feature(point, {\n",
    "            'Latitude': row['Latitude'],\n",
    "            'Longitude': row['Longitude'],\n",
    "            'Crop_Name': row['Crop_Name']\n",
    "        })\n",
    "\n",
    "    features = [row_to_feature(row) for _, row in df.iterrows()]\n",
    "    feature_collection = ee.FeatureCollection(features)\n",
    "\n",
    "    all_fc = []\n",
    "\n",
    "    for date in dates:\n",
    "        start = ee.Date(str(date))\n",
    "        end = start.advance(5, 'day')\n",
    "\n",
    "        s2_sr = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "            .filterDate(start, end) \\\n",
    "            .filterBounds(feature_collection) \\\n",
    "            .select(['B11', 'B12', 'B3', 'B4', 'B8', 'B8A', 'QA60'])\n",
    "\n",
    "        s2_clouds = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY') \\\n",
    "            .filterDate(start, end) \\\n",
    "            .filterBounds(feature_collection)\n",
    "\n",
    "        joined = ee.Join.saveFirst('cloud_mask').apply(\n",
    "            primary=s2_sr,\n",
    "            secondary=s2_clouds,\n",
    "            condition=ee.Filter.equals(leftField='system:index', rightField='system:index')\n",
    "        )\n",
    "\n",
    "        def process_image(img):\n",
    "            img = ee.Image(img)\n",
    "            masked = ee.Image(mask_clouds(img))\n",
    "            return add_indices(masked)\n",
    "\n",
    "        processed = ee.ImageCollection(joined).map(process_image)\n",
    "\n",
    "        # Check if any image is available\n",
    "        if processed.size().lte(0).getInfo():\n",
    "            print(f\"⚠️ No image found for {date}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        img = processed.median()\n",
    "\n",
    "        reduced = img.reduceRegions(\n",
    "            collection=feature_collection,\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            scale=10\n",
    "        )\n",
    "\n",
    "        reduced = reduced.map(lambda f: f.set('Date', str(date)))\n",
    "        all_fc.append(reduced)\n",
    "\n",
    "    if all_fc:\n",
    "        merged_fc = ee.FeatureCollection(all_fc).flatten()\n",
    "\n",
    "        chunk_name = os.path.splitext(filename)[0]\n",
    "        task = ee.batch.Export.table.toDrive(\n",
    "            collection=merged_fc,\n",
    "            description=f'{export_description_prefix}_{chunk_name}_QA_CloudProb',\n",
    "            folder=export_folder,\n",
    "            fileNamePrefix=f'{chunk_name}_QA_CloudProb',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        task.start()\n",
    "        print(f\"✅ Export started for {chunk_name}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No valid images found for any dates in {filename}, skipping export.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a765904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
