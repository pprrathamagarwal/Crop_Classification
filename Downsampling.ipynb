{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7d9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c7da98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rows with Structure = 'Herb' removed and file saved.\n"
     ]
    }
   ],
   "source": [
    "# üìÑ Path to your input CSV file\n",
    "input_file = 'Karnataka_Datasets/Across/Kharif/Cropland_Masked/Train/2018/combined_train.csv'\n",
    "\n",
    "# üì§ Path to save the output\n",
    "output_file = input_file\n",
    "\n",
    "# üîÉ Load CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# ‚ùå Remove rows where Structure is 'Herb'\n",
    "df_filtered = df[df['Structure'] != 'Herb']\n",
    "\n",
    "# üíæ Save the filtered file\n",
    "df_filtered.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ Rows with Structure = 'Herb' removed and file saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1bf9ca",
   "metadata": {},
   "source": [
    "# Downsampling on all the Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced2e6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median group size: 4087.5\n",
      "Upper limit for downsampling: 5313\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Karnataka_Datasets/Across/Kharif/Cropland_Masked/Train/2018/combined_train.csv\")\n",
    "\n",
    "# Group by Structure, Height, and Duration\n",
    "group_cols = ['Structure', 'Height', 'Duration']\n",
    "grouped = df.groupby(group_cols)\n",
    "\n",
    "# Get sizes of each group\n",
    "group_sizes = grouped.size()\n",
    "median_size = group_sizes.median()\n",
    "upper_limit = median_size * 1.3  # Allow 30% variation\n",
    "\n",
    "print(f\"Median group size: {median_size}\")\n",
    "print(f\"Upper limit for downsampling: {int(upper_limit)}\")\n",
    "\n",
    "# Controlled downsampling\n",
    "def controlled_downsample(group):\n",
    "    size = len(group)\n",
    "    if size > upper_limit:\n",
    "        return group.sample(n=int(upper_limit), random_state=42)\n",
    "    return group\n",
    "\n",
    "balanced_df = grouped.apply(controlled_downsample).reset_index(drop=True)\n",
    "\n",
    "# Save or return\n",
    "balanced_df.to_csv(\"Karnataka_Datasets/Across/Kharif/Cropland_Masked/Train/2018/Combined_Train_Balanced.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ad8e5",
   "metadata": {},
   "source": [
    "# Downsampling on a Specific Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe18f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure\n",
      "Grass    16599\n",
      "Root      2777\n",
      "Shrub    11035\n",
      "Tree      5313\n",
      "dtype: int64\n",
      "Minimum group size: 2777\n",
      "Maximum allowed: 4165\n",
      "Structure\n",
      "Grass    4165\n",
      "Shrub    4165\n",
      "Tree     4165\n",
      "Root     2777\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Karnataka_Datasets/Across/Kharif/Cropland_Masked/Train/2018/Combined_Train_Balanced.csv\")\n",
    "\n",
    "# Define the target column\n",
    "target_col = \"Structure\"\n",
    "\n",
    "# Group by Structure\n",
    "grouped = df.groupby(target_col)\n",
    "group_sizes = grouped.size()\n",
    "min_size = group_sizes.min()\n",
    "max_allowed = int(min_size * 1.5)\n",
    "\n",
    "print(group_sizes)\n",
    "print(f\"Minimum group size: {min_size}\")\n",
    "print(f\"Maximum allowed: {max_allowed}\")\n",
    "\n",
    "# Controlled downsampling\n",
    "def downsample(group):\n",
    "    if len(group) > max_allowed:\n",
    "        return group.sample(n=max_allowed, random_state=42)\n",
    "    return group\n",
    "\n",
    "balanced_df = grouped.apply(downsample).reset_index(drop=True)\n",
    "\n",
    "# Check new distribution\n",
    "print(balanced_df['Structure'].value_counts())\n",
    "\n",
    "# Save the balanced dataset\n",
    "balanced_df.to_csv(\"Karnataka_Datasets/Across/Kharif/Cropland_Masked/Train/2018/Combined_Train_Balanced.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a4ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4485c5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed and saved: Karnataka_Datasets/Across/Kharif/2019-20/SAR_GCVI/AEZ_6\\bidar_merged.csv\n",
      "‚úÖ Processed and saved: Karnataka_Datasets/Across/Kharif/2019-20/SAR_GCVI/AEZ_6\\bijapura_merged.csv\n",
      "‚úÖ Processed and saved: Karnataka_Datasets/Across/Kharif/2019-20/SAR_GCVI/AEZ_6\\dharwad_merged.csv\n",
      "‚úÖ Processed and saved: Karnataka_Datasets/Across/Kharif/2019-20/SAR_GCVI/AEZ_6\\raichur_merged.csv\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing CSV files\n",
    "input_dir = 'Karnataka_Datasets/Across/Kharif/2019-20/SAR_GCVI/AEZ_6/'  # ‚Üê change this to your actual path\n",
    "all_files = glob.glob(os.path.join(input_dir, '*.csv'))\n",
    "\n",
    "for file_path in all_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if 'Crop_Name' not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è Skipping {file_path}: 'Crop_Name' column not found.\")\n",
    "            continue\n",
    "\n",
    "        # Count occurrences of each crop\n",
    "        crop_counts = df['Crop_Name'].value_counts()\n",
    "\n",
    "        # Filter crops with at least 250 rows\n",
    "        valid_crops = crop_counts[crop_counts >= 250].index\n",
    "        filtered_df = df[df['Crop_Name'].isin(valid_crops)]\n",
    "\n",
    "        # Save in place\n",
    "        filtered_df.to_csv(file_path, index=False)\n",
    "        print(f\"‚úÖ Processed and saved: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4f34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3147d4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filtered CSV saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "input_path = \"Karnataka_Datasets/Across/Kharif/2020-21/SAR_GCVI/AEZ_6/Final/Train/Train.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Define allowed crop names\n",
    "allowed_crops = [\n",
    "    \"Cotton\", \"Ginger\", \"Greengram\", \"Jowar\", \"Maize\",\n",
    "    \"Paddy\", \"Redgram\", \"Soybean\", \"Sugarcane\", \"Urad\"\n",
    "]\n",
    "\n",
    "# Filter rows\n",
    "filtered_df = df[df['Crop_Name'].isin(allowed_crops)]\n",
    "\n",
    "# Save the result\n",
    "\n",
    "filtered_df.to_csv(input_path, index=False)\n",
    "\n",
    "print(\"‚úÖ Filtered CSV saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb267cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rows with Crop_Name = 'Ginger' removed and file saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "input_path = \"Karnataka_Datasets/Across/Kharif/2020-21/SAR_GCVI/AEZ_6/Final/Test/Bidar_Data_Final.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Define the crop name to remove\n",
    "crop_to_remove = \"Ginger\"  # üîÅ change as needed\n",
    "\n",
    "# Filter out the rows\n",
    "filtered_df = df[df['Crop_Name'] != crop_to_remove]\n",
    "\n",
    "# Save the cleaned file\n",
    "filtered_df.to_csv(input_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Rows with Crop_Name = '{crop_to_remove}' removed and file saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ecc377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downsampling complete and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "input_file=\"Karnataka_Datasets/Across/Kharif/2020-21/SAR_GCVI/AEZ_6/Final/Train/Train.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Split the data by Height\n",
    "medium_df = df[df['Height'] == 'Medium']\n",
    "other_df = df[df['Height'] != 'Medium']\n",
    "\n",
    "# Desired downsample size\n",
    "target_size = 11500\n",
    "\n",
    "# Stratified sampling of \"Medium\" height crops by Crop_Name\n",
    "medium_sampled = (\n",
    "    medium_df.groupby(\"Crop_Name\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(frac=min(1, target_size / len(medium_df) * len(x) / len(x)), random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Combine back with other height categories\n",
    "balanced_df = pd.concat([other_df, medium_sampled], ignore_index=True)\n",
    "\n",
    "# Save result\n",
    "balanced_df.to_csv(input_file, index=False)\n",
    "\n",
    "print(\"‚úÖ Downsampling complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab17e9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Duration balanced: 'Long' downsampled to ~7500 while preserving Crop_Name distribution.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Split the data by Duration\n",
    "long_df = df[df['Duration'] == 'Long']\n",
    "other_df = df[df['Duration'] != 'Long']\n",
    "\n",
    "# Desired downsample size\n",
    "target_size = 7500\n",
    "\n",
    "# Stratified sampling for 'Long' Duration by Crop_Name\n",
    "long_sampled = (\n",
    "    long_df.groupby(\"Crop_Name\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(frac=min(1, target_size / len(long_df)), random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Combine back with 'Low' and 'Medium'\n",
    "balanced_df = pd.concat([other_df, long_sampled], ignore_index=True)\n",
    "\n",
    "# Save the result\n",
    "balanced_df.to_csv(input_file, index=False)\n",
    "\n",
    "print(\"‚úÖ Duration balanced: 'Long' downsampled to ~7500 while preserving Crop_Name distribution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a12b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define limits\n",
    "height_limit = 5000\n",
    "duration_limit = 7500\n",
    "\n",
    "# Function to downsample a group while preserving Crop_Name distribution\n",
    "def stratified_sample(group_df, limit, level_name):\n",
    "    if len(group_df) <= limit:\n",
    "        return group_df\n",
    "    return (\n",
    "        group_df.groupby(\"Crop_Name\", group_keys=False)\n",
    "        .apply(lambda x: x.sample(frac=min(1, limit / len(group_df)), random_state=42))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# Step 1: Downsample each Height group\n",
    "height_balanced = (\n",
    "    df.groupby('Height', group_keys=False)\n",
    "    .apply(lambda g: stratified_sample(g, height_limit, 'Height'))\n",
    ")\n",
    "\n",
    "# Step 2: From the height-balanced data, downsample each Duration group\n",
    "final_balanced = (\n",
    "    height_balanced.groupby('Duration', group_keys=False)\n",
    "    .apply(lambda g: stratified_sample(g, duration_limit, 'Duration'))\n",
    ")\n",
    "\n",
    "# Save the final balanced dataset\n",
    "final_balanced.to_csv(input_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc51aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final duration balancing complete. All Duration categories now have ~3300 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load already height-balanced CSV\n",
    "\n",
    "# Target duration size\n",
    "target_duration = 3300\n",
    "\n",
    "# Stratified sampling by Crop_Name within each Duration\n",
    "def stratified_duration_sample(group):\n",
    "    if len(group) <= target_duration:\n",
    "        return group\n",
    "    return (\n",
    "        group.groupby(\"Crop_Name\", group_keys=False)\n",
    "        .apply(lambda x: x.sample(frac=min(1, target_duration / len(group)), random_state=42))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# Apply sampling to each Duration group\n",
    "duration_balanced = (\n",
    "    df.groupby(\"Duration\", group_keys=False)\n",
    "    .apply(stratified_duration_sample)\n",
    ")\n",
    "\n",
    "# Save final result\n",
    "duration_balanced.to_csv(input_file, index=False)\n",
    "\n",
    "print(\"‚úÖ Final duration balancing complete. All Duration categories now have ~3300 rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0394b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balancing by: Height\n",
      "Minimum group size: 15711\n",
      "Upper limit (min_size * 1.3): 20424\n",
      "\n",
      "Balancing by: Duration\n",
      "Minimum group size: 19029\n",
      "Upper limit (min_size * 1.3): 24737\n",
      "\n",
      "Balancing by: Structure\n",
      "Minimum group size: 1580\n",
      "Upper limit (min_size * 1.3): 2054\n",
      "\n",
      "‚úÖ Files saved in: Karnataka_Datasets/Across/Kharif/Cropland_Masked/Train/2021/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# üìÅ Paths\n",
    "input_path = \"Karnataka_Datasets/Across/Kharif/Cropland_Masked/Train/2021/combined_train.csv\"\n",
    "output_dir = \"Karnataka_Datasets/Across/Kharif/Cropland_Masked/Train/2021/\"\n",
    "\n",
    "# üìÇ Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# üß† Function to balance by a single column using min_size * factor\n",
    "def balance_by_column(df, column, factor=1.3):\n",
    "    grouped = df.groupby(column)\n",
    "    group_sizes = grouped.size()\n",
    "    min_size = group_sizes.min()\n",
    "    upper_limit = int(min_size * factor)\n",
    "\n",
    "    print(f\"\\nBalancing by: {column}\")\n",
    "    print(f\"Minimum group size: {min_size}\")\n",
    "    print(f\"Upper limit (min_size * {factor}): {upper_limit}\")\n",
    "\n",
    "    def controlled_downsample(group):\n",
    "        if len(group) > upper_limit:\n",
    "            return group.sample(n=upper_limit, random_state=42)\n",
    "        return group\n",
    "\n",
    "    balanced = grouped.apply(controlled_downsample).reset_index(drop=True)\n",
    "    return balanced\n",
    "\n",
    "# üìÑ Load data\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# üì¶ Balance by Height\n",
    "height_balanced = balance_by_column(df, 'Height', factor=1.3)\n",
    "height_file = os.path.join(output_dir, \"Height_Balanced.csv\")\n",
    "height_balanced.to_csv(height_file, index=False)\n",
    "\n",
    "# üì¶ Balance by Duration\n",
    "duration_balanced = balance_by_column(df, 'Duration', factor=1.3)\n",
    "duration_file = os.path.join(output_dir, \"Duration_Balanced.csv\")\n",
    "duration_balanced.to_csv(duration_file, index=False)\n",
    "\n",
    "# üì¶ Balance by Structure\n",
    "structure_balanced = balance_by_column(df, 'Structure', factor=1.3)\n",
    "structure_file = os.path.join(output_dir, \"Structure_Balanced.csv\")\n",
    "structure_balanced.to_csv(structure_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Files saved in: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331eb30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
