{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2209ecb0",
   "metadata": {},
   "source": [
    "# All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c7abe",
   "metadata": {},
   "source": [
    "### Find the Max Number of Timesteps for any Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28429ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¢ Time steps per point (Top 10):\n",
      "    Latitude  Longitude  TimeSteps\n",
      "0  13.086502  77.475513         70\n",
      "1  13.086503  77.475486         70\n",
      "2  13.086532  77.473382         70\n",
      "3  13.086533  77.473390         70\n",
      "4  13.086534  77.473396         70\n",
      "5  13.086914  77.472185         70\n",
      "6  13.086940  77.472175         70\n",
      "7  13.086987  77.475678         70\n",
      "8  13.086998  77.472010         70\n",
      "9  13.087005  77.471977         70\n",
      "\n",
      "üìà Max time steps for any point: 70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv('Karnataka_Datasets/Across/Sample/Karnataka_Chunk_10_90.csv')\n",
    "\n",
    "# Convert 'Date' to datetime (format: DD-MM-YYYY)\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "\n",
    "# Drop duplicates to avoid extra time steps\n",
    "df = df.drop_duplicates(subset=['Latitude', 'Longitude', 'Date'])\n",
    "\n",
    "# Sort by Latitude, Longitude, and Date\n",
    "df = df.sort_values(by=['Latitude', 'Longitude', 'Date'])\n",
    "\n",
    "# Set multi-index for easy grouping\n",
    "df.set_index(['Latitude', 'Longitude', 'Date'], inplace=True)\n",
    "\n",
    "# Columns of interest for vegetation indices and bands\n",
    "columns_of_interest = ['NDVI', 'B11', 'B12', 'B3', 'B4', 'B8', 'B8A', 'GCVI']\n",
    "\n",
    "# Bring 'Date' back as a column\n",
    "df = df.reset_index()\n",
    "\n",
    "# Recalculate TimeIndex using date ranking per point\n",
    "df['TimeIndex'] = df.groupby(['Latitude', 'Longitude'])['Date'].rank(method='first').astype(int)\n",
    "\n",
    "# Melt the dataframe to long format for pivoting\n",
    "df_melt = df.melt(id_vars=['Latitude', 'Longitude', 'TimeIndex'], \n",
    "                  value_vars=columns_of_interest, \n",
    "                  var_name='Variable', \n",
    "                  value_name='Value')\n",
    "\n",
    "# Create a new DataFrame that represents every possible (Lat, Lon, TimeIndex) combination\n",
    "max_time_index = df['TimeIndex'].max()\n",
    "time_index_range = pd.DataFrame({'TimeIndex': range(1, max_time_index + 1)})\n",
    "\n",
    "# Create a grid of every Latitude, Longitude and TimeIndex combination\n",
    "lat_lon_combinations = df[['Latitude', 'Longitude']].drop_duplicates()\n",
    "full_grid = pd.merge(lat_lon_combinations, time_index_range, how='cross')\n",
    "\n",
    "# Merge the original melted DataFrame with the full grid to ensure all time indices\n",
    "df_full = pd.merge(full_grid, df_melt, on=['Latitude', 'Longitude', 'TimeIndex'], how='left')\n",
    "\n",
    "# Pivot to get variables as columns (VI/Band_timeIndex)\n",
    "df_pivot = df_full.pivot(index=['Latitude', 'Longitude'], \n",
    "                         columns=['Variable', 'TimeIndex'], \n",
    "                         values='Value')\n",
    "\n",
    "# Reindex columns to ensure all (Variable, TimeIndex) combos exist\n",
    "full_column_index = pd.MultiIndex.from_product(\n",
    "    [columns_of_interest, range(1, max_time_index + 1)],\n",
    "    names=['Variable', 'TimeIndex']\n",
    ")\n",
    "df_pivot = df_pivot.reindex(columns=full_column_index)\n",
    "\n",
    "# Flatten the multi-level columns to something more readable\n",
    "df_pivot.columns = [f'{var}_{i}' for var, i in df_pivot.columns]\n",
    "\n",
    "# Reset the index for final output\n",
    "df_pivot = df_pivot.reset_index()\n",
    "\n",
    "# Merge Crop_Name back to the dataset\n",
    "crop_name_df = df[['Latitude', 'Longitude', 'Crop_Name']].drop_duplicates()\n",
    "df_pivot = df_pivot.merge(crop_name_df, on=['Latitude', 'Longitude'], how='left')\n",
    "\n",
    "# Save the final result\n",
    "df_pivot.to_csv('Karnataka_Datasets/', index=False)\n",
    "\n",
    "# Optional: Print counts of available time steps per (Latitude, Longitude) pair\n",
    "point_counts = df.groupby(['Latitude', 'Longitude']).size().reset_index(name='TimeSteps')\n",
    "print(\"\\nüî¢ Time steps per point (Top 10):\")\n",
    "print(point_counts.head(10))\n",
    "\n",
    "# Optional: Check max time steps\n",
    "print(f\"\\nüìà Max time steps for any point: {point_counts['TimeSteps'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff10c552",
   "metadata": {},
   "source": [
    "### Function: Flatten and Concatenate a CSV Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b402db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Processing: Karnataka_Chunk_10_QA.csv\n",
      "üìÇ Processing: Karnataka_Chunk_1_QA.csv\n",
      "üìÇ Processing: Karnataka_Chunk_2_QA.csv\n",
      "üìÇ Processing: Karnataka_Chunk_3_QA.csv\n",
      "üìÇ Processing: Karnataka_Chunk_4_QA.csv\n",
      "üìÇ Processing: Karnataka_Chunk_5_QA.csv\n",
      "üìÇ Processing: Karnataka_Chunk_6_QA.csv\n",
      "üìÇ Processing: Karnataka_Chunk_7_QA.csv\n",
      "üìÇ Processing: Karnataka_Chunk_8_QA.csv\n",
      "üìÇ Processing: Karnataka_Chunk_9_QA.csv\n",
      "\n",
      "‚úÖ All files processed and saved to: Karnataka_Datasets/Across_QA/90/Karnataka_Merged_S2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_file(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Parse date\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='mixed', dayfirst=True, errors='coerce')\n",
    "\n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates(subset=['Latitude', 'Longitude', 'Date'])\n",
    "\n",
    "    # Sort and reset index\n",
    "    df = df.sort_values(by=['Latitude', 'Longitude', 'Date'])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Assign TimeIndex per (Latitude, Longitude) using date rank\n",
    "    df['TimeIndex'] = df.groupby(['Latitude', 'Longitude'])['Date'].rank(method='first').astype(int)\n",
    "\n",
    "    # Columns to use\n",
    "    columns_of_interest = ['NDVI', 'B11', 'B12', 'B3', 'B4', 'B8', 'B8A', 'GCVI']\n",
    "\n",
    "    # Melt to long format\n",
    "    df_melt = df.melt(id_vars=['Latitude', 'Longitude', 'TimeIndex'], \n",
    "                      value_vars=columns_of_interest, \n",
    "                      var_name='Variable', \n",
    "                      value_name='Value')\n",
    "\n",
    "    # Prepare full grid to ensure all time indices are retained\n",
    "    max_time_index = df['TimeIndex'].max()\n",
    "    time_index_range = pd.DataFrame({'TimeIndex': range(1, max_time_index + 1)})\n",
    "    lat_lon_combinations = df[['Latitude', 'Longitude']].drop_duplicates()\n",
    "    full_grid = pd.merge(lat_lon_combinations, time_index_range, how='cross')\n",
    "\n",
    "    df_full = pd.merge(full_grid, df_melt, on=['Latitude', 'Longitude', 'TimeIndex'], how='left')\n",
    "\n",
    "    # Pivot: one column per variable per time step\n",
    "    df_pivot = df_full.pivot(index=['Latitude', 'Longitude'], \n",
    "                             columns=['Variable', 'TimeIndex'], \n",
    "                             values='Value')\n",
    "\n",
    "    # Reindex columns for missing combinations\n",
    "    full_column_index = pd.MultiIndex.from_product(\n",
    "        [columns_of_interest, range(1, max_time_index + 1)],\n",
    "        names=['Variable', 'TimeIndex']\n",
    "    )\n",
    "    df_pivot = df_pivot.reindex(columns=full_column_index)\n",
    "\n",
    "    # Flatten column names\n",
    "    df_pivot.columns = [f'{var}_{i}' for var, i in df_pivot.columns]\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "\n",
    "    # Merge crop name back\n",
    "    crop_name_df = df[['Latitude', 'Longitude', 'Crop_Name']].drop_duplicates()\n",
    "    df_pivot = df_pivot.merge(crop_name_df, on=['Latitude', 'Longitude'], how='left')\n",
    "\n",
    "    return df_pivot\n",
    "\n",
    "\n",
    "# === Main logic ===\n",
    "\n",
    "input_dir = 'Karnataka_Datasets/Across_QA/90/'  # change this as needed\n",
    "output_path = os.path.join(input_dir, 'Karnataka_Merged_S2.csv')\n",
    "\n",
    "# List all CSV files\n",
    "csv_files = glob(os.path.join(input_dir, '*.csv'))\n",
    "\n",
    "# List to hold all processed DataFrames\n",
    "processed_list = []\n",
    "\n",
    "# Process each CSV\n",
    "for file in csv_files:\n",
    "    print(f'üìÇ Processing: {os.path.basename(file)}')\n",
    "    processed_df = process_file(file)\n",
    "    processed_list.append(processed_df)\n",
    "\n",
    "# Merge all processed files\n",
    "final_df = pd.concat(processed_list, ignore_index=True)\n",
    "\n",
    "# Save final merged result\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(f'\\n‚úÖ All files processed and saved to: {output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b76108",
   "metadata": {},
   "source": [
    "### Step: Reading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b22c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Processing: chunk_1_QA_CloudProb.csv\n",
      "\n",
      "‚úÖ All files processed and saved to: Karnataka_Datasets/Across/Cloud_Prob/Karnataka_Merged_S2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Config ===\n",
    "input_dir = 'Karnataka_Datasets/Across/Cloud_Prob/'  # replace with your folder path\n",
    "output_path = os.path.join(input_dir, 'Karnataka_Merged_S2.csv')\n",
    "\n",
    "# === Parameters ===\n",
    "features = ['NDVI', 'GCVI', 'B11', 'B12', 'B3', 'B4', 'B8', 'B8A']\n",
    "\n",
    "# === Helper Function ===\n",
    "def process_file(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Clean lat/lon\n",
    "    df['Latitude'] = df['Latitude'].round(6)\n",
    "    df['Longitude'] = df['Longitude'].round(6)\n",
    "\n",
    "    # Parse date\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df = df.dropna(subset=['Date'])\n",
    "\n",
    "    # Determine full date range with 5-day interval\n",
    "    min_date, max_date = df['Date'].min(), df['Date'].max()\n",
    "    full_dates = pd.date_range(start=min_date, end=max_date, freq='5D')\n",
    "\n",
    "    # Full lat/lon-date grid\n",
    "    latlon = df[['Latitude', 'Longitude']].drop_duplicates()\n",
    "    full_index = latlon.assign(key=1).merge(pd.DataFrame({'Date': full_dates, 'key': 1}), on='key').drop('key', axis=1)\n",
    "\n",
    "    # Merge and melt\n",
    "    df_merged = pd.merge(full_index, df[['Latitude', 'Longitude', 'Date'] + features],\n",
    "                         on=['Latitude', 'Longitude', 'Date'], how='left')\n",
    "    \n",
    "    df_long = df_merged.melt(id_vars=['Latitude', 'Longitude', 'Date'],\n",
    "                             value_vars=features, var_name='Variable', value_name='Value')\n",
    "\n",
    "    # Pivot to wide format\n",
    "    df_pivot = df_long.pivot_table(index=['Latitude', 'Longitude'],\n",
    "                                   columns=['Variable', 'Date'],\n",
    "                                   values='Value')\n",
    "\n",
    "    # Flatten column names\n",
    "    df_pivot.columns = [f\"{var}_{date.strftime('%Y-%m-%d')}\" for var, date in df_pivot.columns]\n",
    "    df_pivot = df_pivot.reindex(sorted(df_pivot.columns), axis=1)\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "\n",
    "    return df_pivot\n",
    "\n",
    "# === Main logic ===\n",
    "csv_files = glob(os.path.join(input_dir, '*.csv'))\n",
    "all_processed = []\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f'üìÇ Processing: {os.path.basename(file)}')\n",
    "    try:\n",
    "        processed_df = process_file(file)\n",
    "        all_processed.append(processed_df)\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Failed to process {file}: {e}')\n",
    "\n",
    "# Merge and save\n",
    "if all_processed:\n",
    "    final_df = pd.concat(all_processed, ignore_index=True)\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f'\\n‚úÖ All files processed and saved to: {output_path}')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No files were processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8858cb5",
   "metadata": {},
   "source": [
    "### Step: Reading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59f3fd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2021, 4, 1), datetime.date(2021, 4, 6), datetime.date(2021, 4, 11), datetime.date(2021, 4, 16), datetime.date(2021, 4, 21), datetime.date(2021, 4, 26), datetime.date(2021, 5, 1), datetime.date(2021, 5, 6), datetime.date(2021, 5, 11), datetime.date(2021, 5, 16), datetime.date(2021, 5, 21), datetime.date(2021, 5, 26), datetime.date(2021, 5, 31), datetime.date(2021, 6, 5), datetime.date(2021, 6, 10), datetime.date(2021, 6, 15), datetime.date(2021, 6, 20), datetime.date(2021, 6, 25), datetime.date(2021, 6, 30), datetime.date(2021, 7, 5), datetime.date(2021, 7, 10), datetime.date(2021, 7, 15), datetime.date(2021, 7, 20), datetime.date(2021, 7, 25), datetime.date(2021, 7, 30), datetime.date(2021, 8, 4), datetime.date(2021, 8, 9), datetime.date(2021, 8, 14), datetime.date(2021, 8, 19), datetime.date(2021, 8, 24), datetime.date(2021, 8, 29), datetime.date(2021, 9, 3), datetime.date(2021, 9, 8), datetime.date(2021, 9, 13), datetime.date(2021, 9, 18), datetime.date(2021, 9, 23), datetime.date(2021, 9, 28), datetime.date(2021, 10, 3), datetime.date(2021, 10, 8), datetime.date(2021, 10, 13), datetime.date(2021, 10, 18), datetime.date(2021, 10, 23), datetime.date(2021, 10, 28), datetime.date(2021, 11, 2), datetime.date(2021, 11, 12), datetime.date(2021, 11, 17), datetime.date(2021, 11, 22), datetime.date(2021, 11, 27), datetime.date(2021, 12, 2), datetime.date(2021, 12, 7), datetime.date(2021, 12, 17), datetime.date(2021, 12, 22), datetime.date(2021, 12, 27), datetime.date(2022, 1, 1), datetime.date(2022, 1, 6), datetime.date(2022, 1, 11), datetime.date(2022, 1, 16), datetime.date(2022, 1, 21), datetime.date(2022, 1, 26), datetime.date(2022, 1, 31), datetime.date(2022, 2, 5), datetime.date(2022, 2, 10), datetime.date(2022, 2, 15), datetime.date(2022, 2, 20), datetime.date(2022, 2, 25), datetime.date(2022, 3, 2), datetime.date(2022, 3, 7), datetime.date(2022, 3, 12), datetime.date(2022, 3, 17), datetime.date(2022, 3, 22), datetime.date(2022, 3, 27)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('Karnataka_Datasets/Across/Cloud_Prob/chunk_1_QA_CloudProb.csv')\n",
    "\n",
    "# Convert the date column to datetime (if it's not already)\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "# Drop any rows where date conversion failed (optional)\n",
    "df = df.dropna(subset=['Date'])\n",
    "\n",
    "# Print unique dates\n",
    "unique_dates = df['Date'].dt.date.unique()\n",
    "print(sorted(unique_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e564ce61",
   "metadata": {},
   "source": [
    "### Step: Reading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b86b1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('Karnataka_Datasets/Across/Cloud_Prob/90/chunk_1_QA_CloudProb.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Round Latitude/Longitude to handle floating-point precision issues\n",
    "df['Latitude'] = df['Latitude'].round(6)\n",
    "df['Longitude'] = df['Longitude'].round(6)\n",
    "\n",
    "# Generate full 5-day date range\n",
    "date_range = pd.date_range(start='2021-04-01', end='2022-03-27', freq='5D')\n",
    "\n",
    "# Get unique Lat-Long pairs\n",
    "unique_lat_lon = df[['Latitude', 'Longitude']].drop_duplicates()\n",
    "lat_lon_tuples = list(unique_lat_lon.itertuples(index=False, name=None))\n",
    "\n",
    "# Create MultiIndex of all Lat-Long-Date combinations\n",
    "full_index = pd.MultiIndex.from_product(\n",
    "    [lat_lon_tuples, date_range],\n",
    "    names=['Lat-Long', 'Date']\n",
    ")\n",
    "\n",
    "# Convert to DataFrame and split Lat-Long into columns\n",
    "full_df = (\n",
    "    pd.DataFrame(index=full_index)\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        Latitude=lambda x: x['Lat-Long'].apply(lambda ll: ll[0]),  # Use apply for tuples\n",
    "        Longitude=lambda x: x['Lat-Long'].apply(lambda ll: ll[1])\n",
    "    )\n",
    "    .drop(columns='Lat-Long')\n",
    ")\n",
    "\n",
    "# Merge with original data\n",
    "merged_df = pd.merge(\n",
    "    full_df,\n",
    "    df[['Latitude', 'Longitude', 'Date', 'NDVI']],\n",
    "    on=['Latitude', 'Longitude', 'Date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Pivot to wide format\n",
    "pivot_df = merged_df.pivot_table(\n",
    "    index=['Latitude', 'Longitude'],\n",
    "    columns='Date',\n",
    "    values='NDVI',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Reindex columns to include ALL 5-day dates (even if missing in data)\n",
    "pivot_df = pivot_df.reindex(columns=date_range, fill_value=np.nan)\n",
    "\n",
    "# Rename columns to NDVI_YYYY-MM-DD format\n",
    "pivot_df.columns = [f'NDVI_{date.strftime(\"%Y-%m-%d\")}' for date in pivot_df.columns]\n",
    "\n",
    "# Reset index for CSV output\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# Save\n",
    "pivot_df.to_csv('Karnataka_Datasets/Across/Cloud_Prob/90/Karnataka_Merged_S2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012eddfd",
   "metadata": {},
   "source": [
    "### Function: `process_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45b6835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk_10_QA_CloudProb.csv...\n",
      "Processing chunk_11_QA_CloudProb.csv...\n",
      "Processing chunk_12_QA_CloudProb.csv...\n",
      "Processing chunk_13_QA_CloudProb.csv...\n",
      "Processing chunk_14_QA_CloudProb.csv...\n",
      "Processing chunk_15_QA_CloudProb.csv...\n",
      "Processing chunk_16_QA_CloudProb.csv...\n",
      "Processing chunk_1_QA_CloudProb.csv...\n",
      "Processing chunk_2_QA_CloudProb.csv...\n",
      "Merged file saved to Karnataka_Datasets/Across/Cloud_Prob/90/Karnataka_Merged_S2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_csv(file_path, date_range):\n",
    "    \"\"\"Process a single CSV file into the time series format.\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Round coordinates to avoid precision mismatches\n",
    "    df['Latitude'] = df['Latitude'].round(6)\n",
    "    df['Longitude'] = df['Longitude'].round(6)\n",
    "    \n",
    "    # Get unique Lat-Long pairs\n",
    "    unique_lat_lon = df[['Latitude', 'Longitude']].drop_duplicates()\n",
    "    lat_lon_tuples = list(unique_lat_lon.itertuples(index=False, name=None))\n",
    "    \n",
    "    # Create full index of all Lat-Long-Date combinations\n",
    "    full_index = pd.MultiIndex.from_product(\n",
    "        [lat_lon_tuples, date_range],\n",
    "        names=['Lat-Long', 'Date']\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame and split Lat-Long into columns\n",
    "    full_df = (\n",
    "        pd.DataFrame(index=full_index)\n",
    "        .reset_index()\n",
    "        .assign(\n",
    "            Latitude=lambda x: x['Lat-Long'].apply(lambda ll: ll[0]),\n",
    "            Longitude=lambda x: x['Lat-Long'].apply(lambda ll: ll[1])\n",
    "        )\n",
    "        .drop(columns='Lat-Long')\n",
    "    )\n",
    "    \n",
    "    # Merge with original data\n",
    "    merged_df = pd.merge(\n",
    "        full_df,\n",
    "        df[['Latitude', 'Longitude', 'Date', 'NDVI']],\n",
    "        on=['Latitude', 'Longitude', 'Date'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Pivot and reindex to ensure all dates are included\n",
    "    pivot_df = merged_df.pivot_table(\n",
    "        index=['Latitude', 'Longitude'],\n",
    "        columns='Date',\n",
    "        values='NDVI',\n",
    "        aggfunc='first'\n",
    "    ).reindex(columns=date_range, fill_value=np.nan)\n",
    "    \n",
    "    # Rename columns\n",
    "    pivot_df.columns = [f'NDVI_{date.strftime(\"%Y-%m-%d\")}' for date in pivot_df.columns]\n",
    "    return pivot_df.reset_index()\n",
    "\n",
    "# Configuration\n",
    "INPUT_DIR = 'Karnataka_Datasets/Across/Cloud_Prob/90/'  # Directory containing all CSV files\n",
    "OUTPUT_DIR = 'Karnataka_Datasets/Across/Cloud_Prob/90/'          # Directory to save merged result\n",
    "MERGED_FILENAME = 'Karnataka_Merged_S2.csv'\n",
    "\n",
    "# Generate full date range\n",
    "date_range = pd.date_range(start='2021-04-01', end='2022-03-27', freq='5D')\n",
    "\n",
    "# Process all CSV files\n",
    "all_dfs = []\n",
    "for csv_file in glob(os.path.join(INPUT_DIR, '*.csv')):\n",
    "    print(f\"Processing {os.path.basename(csv_file)}...\")\n",
    "    df = process_csv(csv_file, date_range)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Remove duplicates (if any)\n",
    "merged_df = merged_df.drop_duplicates(subset=['Latitude', 'Longitude'], keep='first')\n",
    "\n",
    "# Save merged result\n",
    "merged_df.to_csv(os.path.join(OUTPUT_DIR, MERGED_FILENAME), index=False)\n",
    "print(f\"Merged file saved to {os.path.join(OUTPUT_DIR, MERGED_FILENAME)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
